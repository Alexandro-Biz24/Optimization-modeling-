# **Exploring Optimization:**
Gradient Descent and its Variants
This project explores different optimization techniques based on gradient descent, a key algorithm for training models in machine learning. We compare several optimization methods to assess their efficiency and speed of convergence.

## **Project content**

### **We are implementing and analyzing the following variants:**
Gradient Descent (GD) - Classic gradient descent, updated over the entire data set.
Stochastic Gradient Descent (SGD) - Faster but noisier updating by taking a single sample at each iteration.
Mini-Batch Gradient Descent (MBGD) - A compromise between GD and SGD, using a mini-batch of data to stabilize training.
ADAM (Adaptive Moment Estimation) - Advanced optimizer combining momentum and adaptive learning rate adjustment.

### **Objectives**
Compare convergence speed and accuracy of different methods.
Visualize the evolution of the loss function at each iteration.
Understand the advantages and disadvantages of adaptive methods such as ADAM.

### **Results and comparisons**
We plot loss curves for each optimizer to better understand their behavior and effectiveness in an experimental setting.
